{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "795b6af9-4335-4045-abca-03a3f194a62b",
   "metadata": {},
   "source": [
    "# 深度学习网络实验——以VGG和ResNet为例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fee5b57-f0a0-4c9f-a8ab-2631f7338b49",
   "metadata": {},
   "source": [
    "# 一、 VGG网络介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1408b078-f85f-41c1-b587-de814bd074ad",
   "metadata": {},
   "source": [
    "<font size=4> VGG网络是经典的卷积神经网络之一，主要的思想是用连续的多个$3\\times3$的卷积核代替单层的$5\\times5$、$7\\times7$卷积核，从而达到减少网络参数、加深网络深度的目的。根据不同的用法，VGG网络有着不同的网络结构，如下图所示："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ad2eed-31fa-4850-abb5-81a62b7ff83c",
   "metadata": {},
   "source": [
    "![网络结构参数](pic/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9199a3-2f8c-42eb-ad8a-6fe59db1253c",
   "metadata": {},
   "source": [
    "<font size=4> 图片中，不同的列表示不同规模的VGG网络各层结构。以第D列为例，这一列的参数表示了VGG16网络的参数，包括通道数从64到512的13个卷积层、3个全连接层和穿插其中的最大池化层等。具有可训练参数的网络层是卷积层和全连接层，一共有16个，所以叫做VGG16网络。在搭建网络中，还会在卷积层和全连接层后添加归一化和激活函数，这是神经网络的常规操作，因此图片中并不展示出来。VGG16的网络结构可以想象成这个样子："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cd1810-7c9d-4f5b-9292-8cb4d924a958",
   "metadata": {},
   "source": [
    "![网络结构](pic/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058f024c-4cfd-4e23-a6ac-ec0b38c0e57d",
   "metadata": {},
   "source": [
    "# 二、ResNet网络介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5ade5b-23c6-4335-989d-675c3b674fa2",
   "metadata": {},
   "source": [
    "<font size=4>ResNet网络也是经典的卷积网络之一。与VGG中笔直的数据流不同，在ResNet中，独有着“残差连接”这个概念。在某些层中，网络会将未经运算的数据直接加到经过运算的结果上，这部分数据就叫做“残差”。因此，ResNet与其他卷积神经网络相比，更注重了线性变换与非线性变换的平衡，在各种任务中取得了很好的效果，是现在最常用的神经网络之一。常见的ResNet网络有着不同的规格，如ResNet18、ResNet34、ResNet50等。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a2434f-8a8e-43d8-9671-287e63923316",
   "metadata": {},
   "source": [
    "![网络结构参数](pic/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6208d61a-9a18-4c38-8d59-ad10bd87b230",
   "metadata": {},
   "source": [
    "# 三、神经网络的搭建"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3af449-a9cb-4610-9c2b-ebd75470dd9e",
   "metadata": {},
   "source": [
    "<font size=4>在MindSpore框架中，一个神经网络的搭建一般遵循这样的模板："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b413cf0b-bb9c-4137-aa85-dde8b113fd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.nn as nn\n",
    "\n",
    "\n",
    "class Net(nn.Cell):\n",
    "    def __init__(self, Args):  # 必要\n",
    "        super(Net, self).__init__()\n",
    "        \"\"\" 在这里定义所需要的网络层\n",
    "        ...\n",
    "        ..\n",
    "        .\n",
    "        \"\"\"\n",
    "\n",
    "    def block(self, Args):  # 不必要\n",
    "        \"\"\" 若网络结构具有重复性，可以在这种函数中将重复的几个层打包成一个块，在上面的函数里调用，避免繁杂的编写\n",
    "        ...\n",
    "        ..\n",
    "        .\n",
    "        \"\"\"\n",
    "\n",
    "    def construct(self, x):  # 必要\n",
    "        \"\"\" 在这里使用定义好的网络层构建网络\n",
    "        ...\n",
    "        ..\n",
    "        .\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650c49ad-9e27-4f74-a613-fb855cc97736",
   "metadata": {},
   "source": [
    "<font size=4>本例程通过搭建简化的VGG网络：VGG7来具体描述这个模板（参照VGG7.py）。在VGG中，每个卷积层后面都会跟随归一化和激活函数，因此，如果把这三个部分写成一个网络层，就可以让代码更加精简清爽，方便复杂网络的搭建。如VGG7.py中的代码所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02921b79-b807-49c4-a444-fcbb55fe6f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(self, in_channel, out_channel):\n",
    "    seq = nn.SequentialCell(\n",
    "        [\n",
    "            nn.Conv2d(in_channel, out_channel, 3, padding=1, pad_mode='pad'),\n",
    "            nn.BatchNorm2d(out_channel),\n",
    "            nn.ReLU()\n",
    "        ])\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db854c41-d36d-4891-85f8-138ae81f8e3a",
   "metadata": {},
   "source": [
    "<font size=4>nn.SequentialCell() 函数可以提供一个封装的功能，将参数列表里的多个网络层按顺序封装成一个层。调用这个函数，就会按顺序产生Conv2d、BatchNorm2d、ReLu三个网络结构，好比将三块小积木首尾拼接成一个稍大的积木一样。这样，在__init__() 函数中，就可以调用这个函数搭建网络。类似的，这样构建的网络层也可以再次成为nn.SequentialCell() 的参数，参与构建更大的网络层，好比用稍大的积木拼接出更大的积木一样。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d431c4-d4c0-46df-9da1-91a175f88725",
   "metadata": {},
   "source": [
    "<font size=4>这种写在class里的网络块结构也经常被拆出来，作为另一个class来定义。注意观察上面class的定义，只要在class定义的第一行括号里，继承上nn.Cell属性，就可以被视为一种网络结构，既可以被训练，也可以被调用来组成更大的网络。在本例程的ResNet12.py文件中，就是将组成大网络ResNet12的基本单元ResBlock作为一个小的网络，单独用一个class定义的。ResNet12由四个残差块（就是基本单元）组成，每个残差块的结构都一样，由3层卷积层组成。根据相关文献，每个残差块的结构如下图所示："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceffb0d-b7ee-45c1-a664-2942d86777df",
   "metadata": {},
   "source": [
    "![网络结构](pic/4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86431bdf-d025-4d15-acdc-6d57a0614f2a",
   "metadata": {},
   "source": [
    "因此，在Resnet12.py文件中，残差块的结构定义如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a0f642-8df2-4a8a-ac9c-eb820461511c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Cell):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.seq = nn.SequentialCell(\n",
    "            [\n",
    "                nn.Conv2d(in_channel, out_channel, 3, stride=1, padding=1, pad_mode='pad'),\n",
    "                nn.BatchNorm2d(out_channel),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(out_channel, out_channel, 3, stride=1, padding=1, pad_mode='pad'),\n",
    "                nn.BatchNorm2d(out_channel),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(out_channel, out_channel, 3, stride=1, padding=1, pad_mode='pad'),\n",
    "                nn.BatchNorm2d(out_channel),\n",
    "            ])\n",
    "        self.shortup = nn.SequentialCell(\n",
    "            [\n",
    "                nn.Conv2d(in_channel, out_channel, 1, stride=1),\n",
    "                nn.BatchNorm2d(out_channel),\n",
    "            ])\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def construct(self, x):\n",
    "        y = self.seq(x) + self.shortup(x)\n",
    "        y = self.relu(y)\n",
    "        return self.max_pool(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12b015c-2281-4735-a0f4-a7db090b5037",
   "metadata": {},
   "source": [
    "<font size=4>construct() 函数定义网络运行时数据是如何流过网络的。这个函数的输入一般是数据，比如图片数据张量和时序数据张量等。在较为简单的网络中，数据一般按顺序经过不同的网络层，最终得到网络的结果。如VGG7.py中的代码所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d42e42e-d150-4794-ba91-fe08de04f2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct(self, x):\n",
    "    # 使用定义好的运算构建前向网络\n",
    "    x = self.conv1(x)\n",
    "    x = self.max_pool2d(x)\n",
    "\n",
    "    x = self.conv2(x)\n",
    "    x = self.max_pool2d(x)\n",
    "\n",
    "    x = self.conv3(x)\n",
    "    x = self.max_pool2d(x)\n",
    "\n",
    "    x = self.conv4(x)\n",
    "    x = self.max_pool2d(x)\n",
    "\n",
    "    x = self.conv5(x)\n",
    "    x = self.max_pool2d(x)\n",
    "\n",
    "    x = self.flatten(x)\n",
    "    x = self.fc9(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.fc10(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.fc11(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c0075d-e561-4a8c-82f6-4a0728588a73",
   "metadata": {},
   "source": [
    "<font size=4>输入数据x依次经过定义的conv1、maxpool2d、conv2直到fc11等网络层后，得到网络的输出结果。若网络执行分类任务，那么将这些结果拿去经过softmax等处理后，就可以得到分类标签。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f26afd-8a9b-4d44-8fe1-97e37ee281f6",
   "metadata": {},
   "source": [
    "<font size=4>将神经网络视作一个函数，那么我们输入图片、时序等数据，就可以得到分类预测标签、未来预测趋势等结果。这是网络的前向过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff33cb65-44a5-4a73-a123-f5ae279e248a",
   "metadata": {},
   "source": [
    "<font size=4>在训练阶段，根据预测结果和真实结果的差距，可以计算出网络训练的损失。通过梯度反向传播技术，可以根据损失修改网络的参数，从而使网络的结果更接近我们想要的结果。这是网络的训练过程。我们可以设置训练的参数，例如训练轮数等，来让网络训练到让人满意为止。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7965763-1e60-42b1-813e-89f7421ccf76",
   "metadata": {},
   "source": [
    "# 四、神经网络的训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf93369-7e60-47e4-ba88-5ec0542983e4",
   "metadata": {},
   "source": [
    "<font size=4> 神经网络的训练，必然包括网络定义、数据集定义、训练脚本的编写这些部分。关于数据集的定义，这里不再赘述。关于训练脚本的编写，对于不同的深度学习框架，有着不同的编写结构；即便同一个框架，也有着不同的写法。对于MindSpore框架，可以调用其内部定义的高级API来实现网络的训练。具体的写法可以参考main.ipynb里的train()函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fea6a12-adac-4617-a571-8532a6c5498a",
   "metadata": {},
   "source": [
    "<font size=4> 在main.ipynb中，已经写好了数据集部分和训练的脚本。train() 函数具有两个参数，第一个参数控制网络采用哪种结构，第二个参数控制训练的轮数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89b5d53-3434-4cf9-a86e-55d8e9731163",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\"vgg7\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ddd3b7-3b6b-4ead-b8c0-9e7e4da981c7",
   "metadata": {},
   "source": [
    "<font size=4> 这行代码就表示，将网络定义为vgg7网络，并训练1轮。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510a5238-ef67-4943-a8a3-b8dbcb5dd343",
   "metadata": {},
   "source": [
    "# 五、实验内容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3242d630-00f0-4361-9f5d-25e7724d1355",
   "metadata": {},
   "source": [
    "## 任务一 对照第一章中的VGG11参数，补全残缺的VGG11.py文件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60225584-9eea-4d3a-b62f-2fb27544eb31",
   "metadata": {},
   "source": [
    "<font size=4> 本例程已经包含VGG7和ResNet12两个神经网络的样例。请注意观察本例程第一章中VGG网络结构参数的图片，VGG7就是将VGG11中第3、4、5个卷积阶段中重复的层去掉，并去掉一个全连接层实现的。一共去掉了4个层，因此从VGG11变为了VGG7。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff383ff4-c11b-47e2-b4cd-cabf2a4bc39f",
   "metadata": {},
   "source": [
    "<font size=4> 可以参考和复制VGG7.py文件的代码，对缺少的层进行补全。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b714929e-0993-4683-98c1-85d8e0d5b948",
   "metadata": {},
   "source": [
    "## 任务二 对照下面的网络结构，补全残缺的Resnet18.py文件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc74411-0e9a-4018-be36-06ad02e66f36",
   "metadata": {},
   "source": [
    "<font size=4> Resnet18中有两种不同的残差块，它们的结构如下所示："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b998ba-5d0b-4ebb-a7fd-524eae644373",
   "metadata": {},
   "source": [
    "![网络结构](pic/5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4e78b6-d899-410e-99ae-c4d798e25ff4",
   "metadata": {},
   "source": [
    "<font size=4> 在Resnet18.py文件中，左边的残差块被ResBlock实现，右边的残差块被ResBlockDS实现。请参考Resnet12中对残差块的实现过程和代码，对以上两个残差块的代码进行补全。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dfc873-dcc5-4110-9358-35c1f0a89988",
   "metadata": {},
   "source": [
    "# 任务三 训练补全的VGG11和Resnet18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d141412f-cfd5-4397-ba4c-0792a3406ed4",
   "metadata": {},
   "source": [
    "<font size=4> train函数的头几行代码，是根据不同的输入参数初始化不同的网络模型。先仿照倒数两个import语句，在文件的开头import入你写好的两个模型。然后在train函数里添加上VGG11和Resnet18的初始化语句。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5209ab1d-a15b-4688-8e99-2d7180370a83",
   "metadata": {},
   "source": [
    "<font size=4> 最后，对两个模型各训练3个epoch，看到训练结果和训练图线。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
